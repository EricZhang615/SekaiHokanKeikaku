a) 在特定区域内，通过仿真实现基于视觉传感器的环境地图三维重建、匹配与更新，详细功能要求和应用场景如下：
1、 车辆行驶环境大小为XXXm2，环境内可运行车辆不低于30辆；仿真场景内包含多个房间、电梯、通道、货架、货物、车辆、人员及随机存放的物品等元素；
2、 根据仿真场景大小、固定设施造成的视野盲区、传感器视场范围等因素在环境中固定布设视觉传感器，实现场景的全覆盖；
3、 具备将多个视觉传感器的图像进行自适应融合拼接的功能，实现对行驶环境的三维重建，构建全域地图，该步骤离线完成；
4、 每个无人车辆均安装视觉传感器，仿真环境中可通过键盘控制车辆行驶，对局部环境进行信息感知，建立局部地图，完成车辆的初步定位，确定每个无人车辆在全域地图的局部搜索范围并采用信息特征匹配算法进行匹配，从而获取车辆在全域地图下的精准定位；
5、 当车辆端感知到预先建立的全域地图中可通行区域出现变化，如临时堆放的物品、故障车辆阻塞道路时，应及时对全域地图进行更新；
6、 构建的地图应支持路径规划与避障等功能的实现。


# 基于视觉传感器的实时三维重建和室内定位

通过多个摄像头进行三维空间重建，并对摄像头的位置进行定位，并实时更新三维重建。

根据网上的信息¹²³，在室内空间的三维模型中通过视觉传感器定位的方法有以下几种：

- 视觉导航：利用摄像头采集环境图像，通过图像处理和计算机视觉技术，实现对环境的感知和定位。视觉导航具有成本低、精度高、适应性强的优点，但也存在易受光照、遮挡等因素影响、计算量大、输出频率低等缺点。
- 视觉 SLAM：即同时定位与地图构建（Simultaneous Localization and Mapping），是一种利用视觉传感器在未知环境中实时建立三维地图并估计自身位置和姿态的技术。视觉 SLAM 可以实现对室内空间的三维重建和精确定位，但也面临着旋转运动或运动速率加快时定位易失败等问题。
- 视觉与 IMU 融合：IMU 即惯性测量单元（Inertial Measurement Unit），是一种能够输出六自由度（6 DoF）测量信息的传感器。通过将视觉 SLAM 与 IMU 得到的位姿估计结果进行融合，可以解决视觉位姿估计输出频率低、鲁棒性差等问题，提高定位效果。

https://github.com/siyandong/awesome-visual-localization
这是一个收集了视觉定位资源的列表，包括论文、数据集、代码和工具等。该列表涵盖了不同类型的视觉定位方法，如基于检索、基于回归、基于结构、基于语义等，也包括了不同场景和传感器的视觉定位应用，如室内、室外、动态、长期等。

1. SLAM

教学 https://github.com/AlbertSlam/Lee-SLAM-source

https://github.com/tum-vision/lsd_slam

https://github.com/raulmur/ORB_SLAM2

https://github.com/dz306271098/ORB_SLAM3

通过单目、双目、RGBD、惯性传感器，建立空间模型，并跟踪路径和定位。但项目本身是用于单个机器人形成一张地图，如何融合同时运行的多个机器人。

构建点云
https://blog.csdn.net/qq_34935373/article/details/124253180

融合最复杂 如何合并地图 多个机器人互相拍到了 要不要剔除

https://github.com/HKUST-Aerial-Robotics/FUEL

https://github.com/VIS4ROB-lab/ccm_slam 单摄像头

https://github.com/ethz-asl/maplab 开源 协作 单目+惯性

2. 3d models

http://www.ok.sc.e.titech.ac.jp/INLOC/